# CLIP-ICM
![Static Badge](https://img.shields.io/badge/ICML25-yellow)
![Static Badge](https://img.shields.io/badge/to_be_continue-orange)
![Stars](https://img.shields.io/github/stars/ML-TASA/CLIP-ICM)

This repository provides Pytorch implementation for [ICML2025] Learning Invariant Causal Mechanism from Vision-Language Models.

<p align="center">
<img src=".\figs\figure3.png" height = "320" alt="" align=center />
<br><br>
<b>Figure 1.</b> Overview of CLIP-ICM.
</p>


### Quick Start

```bash
# create env
conda create -n clip-icm python=3.9 -y
conda activate clip-icm

# install deps
pip install -r requirements.txt          
````


### Directory Layout

```text
â”œâ”€â”€ CLIP/               # CLIP model implementation and related files
â”œâ”€â”€ DomainBed/          # Domain generalization benchmark
â”œâ”€â”€ clip_icm.py         # CLIP ICM-related functionality
â”œâ”€â”€ converter_domainbed.py # DomainBed data conversion utilities
â”œâ”€â”€ engine.py           # Training engine
â”œâ”€â”€ imagenet_stubs.py   # ImageNet stubs for testing
â”œâ”€â”€ main.py             # Main entry point for the project
â”œâ”€â”€ README.md           # Project-level README
â”œâ”€â”€ requirements.txt    # Python dependencies for the project
â”œâ”€â”€ utils.py            # Utility functions
```



### Citation

If you find our work and codes useful, please consider citing our paper and star our repository (ðŸ¥°ðŸŽ‰Thanks!!!):

```bibtex
@inproceedings{songLearningInvariantCausal2025,
  title = {Learning {{Invariant Causal Mechanism}} from {{Vision-Language Models}}},
  booktitle = {Forty-Second {{International Conference}} on {{Machine Learning}}},
  author = {Song, Zeen and Zhao, Siyu and Zhang, Xingyu and Li, Jiangmeng and Zheng, Changwen and Qiang, Wenwen},
  year = {2025},
  month = may,
  urldate = {2025-06-06},
  langid = {english}
}
```


